# Sentiment Analysis with BERT Neural Network and Python
## BERTSentiment_main_Sentiment.ipynb
Can't be bothered building a model from scratch?

Transformers allows you to easily leverage a pre-trained BERT neural network to do exactly that!

In this notebook we'll go through how to get up and running with Hugging Face Transformers and BERT to be able to calculate sentiment. We'll run the model using a single prompt but also leverage BeautifulSoup to scrape reviews from Yelp to be able to calculate sentiment on a larger scale. 

In this Notebook you'll learn how to: 
1. Install Transformers
2. Scrape reviews from Yelp and Calculate their Sentiment.
3. Perform Sentiment Scoring using BERT and Python

## üîç Mini Project: Fine-Tuning BERT-base-uncased on IMDB Dataset for Sentiment Analysis üé¨‚ú®

Data Preprocessing: Loaded IMDB dataset, cleaned reviews, and tokenized text using BERT tokenizer.
Model Architecture: Utilized the pre-trained BERT-base-uncased model from Hugging Face for transfer learning.
Training: Fine-tuned BERT for sentiment classification, optimizing for binary classification (positive/negative).
Loss & Optimization: Used cross-entropy loss and AdamW optimizer with learning rate scheduling.
Evaluation: Achieved high accuracy by validating the model on test data with precision, recall, and F1-score metrics.
Deployment Ready: Model is scalable for real-world applications in NLP sentiment analysis.
This mini-project demonstrates how to utilize BERT for sentiment analysis‚Äîperfect for hands-on learning! üöÄ

